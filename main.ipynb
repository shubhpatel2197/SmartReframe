{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foh1V0aEq2FL"
   },
   "source": [
    "# **Content-Aware Video Cropping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyeOz6vDrCQg"
   },
   "source": [
    "The logic behind content-aware video cropping involves applying object detection to each frame and prioritizing specific objects to ensure the crop focuses on the most relevant subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ra5pg9OS0TSz",
    "outputId": "2a18e159-7dcf-4618-98ad-a833d0e62258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.28-py3-none-any.whl.metadata (35 kB)\n",
      "Collecting pyDeepInsight\n",
      "  Downloading pyDeepInsight-0.0.1-py3-none-any.whl.metadata (738 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.0+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->torchvision) (3.0.2)\n",
      "Downloading ultralytics-8.3.28-py3-none-any.whl (881 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyDeepInsight-0.0.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyDeepInsight, ultralytics-thop, ultralytics\n",
      "Successfully installed pyDeepInsight-0.0.1 ultralytics-8.3.28 ultralytics-thop-2.0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if event.key is 'enter':\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchvision ultralytics pyDeepInsight\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time,shutil\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip,ImageSequenceClip\n",
    "import tempfile\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6TLX95srGz5"
   },
   "source": [
    "I'm using the YOLOv8l model for object detection, chosen for its speed and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FxuPZ9M61R-o"
   },
   "outputs": [],
   "source": [
    "def setup_yolo():\n",
    "\n",
    "    # Load YOLOv8 model\n",
    "    model = YOLO('yolov8l.pt')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcwwynsfsMbo"
   },
   "source": [
    "This function, detect_objects extracts bounding box coordinates, confidence scores, and class information for each detected object. The output is a list of detections with relevant details, allowing further processing for content-aware cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CbhbBSHB1gjD"
   },
   "outputs": [],
   "source": [
    "def detect_objects(frame: np.ndarray, model: YOLO) -> List[Dict]:\n",
    "    # Run inference\n",
    "    results = model(frame, verbose=False)[0]  # First result from the list\n",
    "\n",
    "    # Extract detections\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        # Get box coordinates (convert to int)\n",
    "        x1, y1, x2, y2 = [int(x) for x in box.xyxy[0].tolist()]\n",
    "\n",
    "        # Get confidence and class\n",
    "        conf = float(box.conf[0])\n",
    "        cls = int(box.cls[0])\n",
    "\n",
    "        detection = {\n",
    "            'bbox': [x1, y1, x2, y2],\n",
    "            'confidence': conf,\n",
    "            'class': cls,\n",
    "            'class_name': results.names[cls]\n",
    "        }\n",
    "        detections.append(detection)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHg3feLUrNFG"
   },
   "source": [
    "The `SubjectTracker` class implements sophisticated tracking logic to maintain consistent focus on subjects across video frames.\n",
    "\n",
    "If score has difference more than 'Threshold' then only switch subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0uO6zm_lossp"
   },
   "outputs": [],
   "source": [
    "class SubjectTracker:\n",
    "\n",
    "    def __init__(self,\n",
    "                 switch_threshold: float = 0.3,    # Minimum score difference to switch subjects\n",
    "                 persistence_bonus: float = 0.25,  # Bonus for sticking with current subject\n",
    "                 history_size: int = 5):\n",
    "        self.current_subject_id = None\n",
    "        self.current_subject_score = 0\n",
    "        self.frame_history = {}  # Track how long we've followed each subject\n",
    "        self.switch_threshold = switch_threshold\n",
    "        self.persistence_bonus = persistence_bonus\n",
    "        self.history_size = history_size\n",
    "        self.last_position = None\n",
    "\n",
    "    def should_switch_subject(self,\n",
    "                            current_score: float,\n",
    "                            new_score: float,\n",
    "                            frames_tracked: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if we should switch to a new subject based on decision rules.\n",
    "        \"\"\"\n",
    "        # Add bonus for currently tracked subject based on duration\n",
    "        duration_bonus = min(frames_tracked * 0.05, 0.2)  # Up to 0.2 bonus for 4+ frames\n",
    "        adjusted_current = current_score + self.persistence_bonus + duration_bonus\n",
    "\n",
    "        # Require significant improvement to switch\n",
    "        return new_score > (adjusted_current + self.switch_threshold)\n",
    "\n",
    "    def update_and_select(self, scored_detections: List[Dict], frame_shape: Tuple[int, int]) -> Dict:\n",
    "        \"\"\"\n",
    "        Update tracking state and select subject based on decision rules.\n",
    "        \"\"\"\n",
    "        if not scored_detections:\n",
    "            self.current_subject_id = None\n",
    "            return None\n",
    "\n",
    "        height, width = frame_shape[:2]\n",
    "        current_frame_subjects = {det['id']: det for det in scored_detections}\n",
    "\n",
    "        # Update frame history for all detected subjects\n",
    "        for subject_id in current_frame_subjects:\n",
    "            if subject_id not in self.frame_history:\n",
    "                self.frame_history[subject_id] = 0\n",
    "            self.frame_history[subject_id] += 1\n",
    "\n",
    "        # Clean up old subjects\n",
    "        self.frame_history = {k: v for k, v in self.frame_history.items()\n",
    "                            if v > 0 and k in current_frame_subjects}\n",
    "\n",
    "        # If we have a current subject and it's still detected\n",
    "        if self.current_subject_id and self.current_subject_id in current_frame_subjects:\n",
    "            current_detection = current_frame_subjects[self.current_subject_id]\n",
    "            current_score = current_detection['score']\n",
    "\n",
    "            # Find best alternative subject\n",
    "            other_detections = [det for det in scored_detections\n",
    "                              if det['id'] != self.current_subject_id]\n",
    "\n",
    "            if other_detections:\n",
    "                best_alternative = max(other_detections, key=lambda x: x['score'])\n",
    "\n",
    "                # Check if we should switch based on decision rules\n",
    "                if not self.should_switch_subject(\n",
    "                    current_score,\n",
    "                    best_alternative['score'],\n",
    "                    self.frame_history[self.current_subject_id]\n",
    "                ):\n",
    "                    return current_detection\n",
    "            else:\n",
    "                return current_detection\n",
    "\n",
    "        # If we reach here, either we don't have a current subject\n",
    "        # or we decided to switch subjects\n",
    "        best_detection = max(scored_detections, key=lambda x: x['score'])\n",
    "        self.current_subject_id = best_detection['id']\n",
    "        self.current_subject_score = best_detection['score']\n",
    "        return best_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COxc95VKs_Js"
   },
   "source": [
    "The `ObjectScorer` class implements a multi-criteria scoring system for detected objects. Scoring is based on:\n",
    "\n",
    "1.   Distance Score (30%)\n",
    "2.   Size Score (25%)\n",
    "3.   Time Score (20%)\n",
    "4.   Class Score (15%)\n",
    "5.   Movement Score (10%)\n",
    "\n",
    "We can change weights of each criteria as per requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K0mLUNXK1i2q"
   },
   "outputs": [],
   "source": [
    "class ObjectScorer:\n",
    "\n",
    "    def __init__(self, frame_width: int, frame_height: int):\n",
    "        self.frame_width = frame_width\n",
    "        self.frame_height = frame_height\n",
    "        self.history = {}  # Track objects over time\n",
    "        self.frame_count = 0\n",
    "        self.fps = 30  # Assuming 30 fps, adjust as needed\n",
    "\n",
    "    def calculate_distance_score(self, center_x: float) -> float:\n",
    "        # Normalize distance from center to [0, 1]\n",
    "        rel_distance = abs(center_x - self.frame_width/2) / (self.frame_width/2)\n",
    "        if rel_distance < 0.33:\n",
    "            return 1.0  # Center third\n",
    "        elif rel_distance < 0.66:\n",
    "            return 0.7  # Middle third\n",
    "        else:\n",
    "            return 0.4  # Outer third\n",
    "\n",
    "    def calculate_size_score(self, bbox: List[float]) -> float:\n",
    "        height = bbox[3] - bbox[1]\n",
    "        rel_height = height / self.frame_height\n",
    "\n",
    "        if 0.3 <= rel_height <= 0.5:\n",
    "            return 1.0\n",
    "        elif 0.2 <= rel_height < 0.3 or 0.5 < rel_height <= 0.6:\n",
    "            return 0.7\n",
    "        elif 0.1 <= rel_height < 0.2:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return 0.2\n",
    "\n",
    "    def calculate_time_score(self, object_id: str) -> float:\n",
    "        if object_id not in self.history:\n",
    "            self.history[object_id] = 1\n",
    "            return 0.2\n",
    "\n",
    "        duration = self.history[object_id] / self.fps  # Convert frames to seconds\n",
    "\n",
    "        if duration > 3:\n",
    "            return 1.0\n",
    "        elif duration > 2:\n",
    "            return 0.7\n",
    "        elif duration > 1:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return 0.2\n",
    "\n",
    "    def calculate_class_score(self, class_name: str) -> float:\n",
    "        if class_name.lower() in ['duck','bird','dog','car','aeroplane','person']:\n",
    "            return 1.0\n",
    "        elif class_name.lower() in ['truck', 'bicycle']:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.2\n",
    "\n",
    "    def calculate_movement_score(self, object_id: str, current_pos: float) -> float:\n",
    "        if object_id not in self.history:\n",
    "            return 0.4  # Default score for new objects\n",
    "\n",
    "        positions = self.history.get(f\"{object_id}_positions\", [])\n",
    "        if not positions:\n",
    "            return 0.4\n",
    "\n",
    "        # Calculate average velocity\n",
    "        velocities = [abs(positions[i] - positions[i-1]) for i in range(1, len(positions))]\n",
    "        if not velocities:\n",
    "            return 0.7\n",
    "\n",
    "        avg_velocity = sum(velocities) / len(velocities)\n",
    "        if avg_velocity < 5:\n",
    "            return 1.0  # Steady movement\n",
    "        elif avg_velocity < 10:\n",
    "            return 0.7  # Slow movement\n",
    "        elif avg_velocity < 20:\n",
    "            return 0.4  # Moderate movement\n",
    "        else:\n",
    "            return 0.2  # Fast/erratic movement\n",
    "\n",
    "    def calculate_final_score(self, detection: Dict, object_id: str) -> float:\n",
    "        bbox = detection['bbox']\n",
    "        center_x = (bbox[0] + bbox[2]) / 2\n",
    "\n",
    "        distance_score = self.calculate_distance_score(center_x)\n",
    "        size_score = self.calculate_size_score(bbox)\n",
    "        time_score = self.calculate_time_score(object_id)\n",
    "        class_score = self.calculate_class_score(detection['class_name'])\n",
    "        movement_score = self.calculate_movement_score(object_id, center_x)\n",
    "\n",
    "        final_score = (\n",
    "            0.20 * distance_score +  # Increased from 0.30\n",
    "             0.35 * size_score +\n",
    "            0.15 * time_score +  # Decreased from 0.20\n",
    "            0.25 * class_score +\n",
    "            0.10 * movement_score\n",
    "        )\n",
    "\n",
    "        # Update history\n",
    "        self.history[object_id] = self.history.get(object_id, 0) + 1\n",
    "        positions = self.history.get(f\"{object_id}_positions\", [])\n",
    "        positions.append(center_x)\n",
    "        if len(positions) > 10:  # Keep last 10 positions\n",
    "            positions.pop(0)\n",
    "        self.history[f\"{object_id}_positions\"] = positions\n",
    "\n",
    "        return final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z4UJE8ttWIO"
   },
   "source": [
    "The `calculate_crop_window` function determines optimal crop positioning based on scores.\n",
    "\n",
    "x1 and x2 represent the left and right horizontal boundaries of the cropping window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sfAm1EPa1l-f"
   },
   "outputs": [],
   "source": [
    "def calculate_crop_window(frame_shape: Tuple[int, int],\n",
    "                         detections: List[Dict],\n",
    "                         object_scorer: ObjectScorer,\n",
    "                         subject_tracker: SubjectTracker,\n",
    "                         target_aspect_ratio: float = 9/16) -> Tuple[int, int, int, int]:\n",
    "\n",
    "    height, width = frame_shape[:2]\n",
    "    target_width = int(height * target_aspect_ratio)\n",
    "\n",
    "    # Score all detections\n",
    "    scored_detections = []\n",
    "    for i, det in enumerate(detections):\n",
    "        object_id = f\"obj_{i}\"\n",
    "        score = object_scorer.calculate_final_score(det, object_id)\n",
    "        scored_detections.append({\n",
    "            'id': object_id,\n",
    "            'score': score,\n",
    "            'bbox': det['bbox']\n",
    "        })\n",
    "\n",
    "    # Apply decision rules through subject tracker\n",
    "    selected_subject = subject_tracker.update_and_select(scored_detections, frame_shape)\n",
    "\n",
    "    if not selected_subject:\n",
    "        # Default to center crop if no subjects detected\n",
    "        x_center = width // 2\n",
    "        x1 = max(0, x_center - target_width // 2)\n",
    "        x2 = min(width, x1 + target_width)\n",
    "        return (x1, 0, x2, height)\n",
    "\n",
    "    # Calculate crop window based on selected subject\n",
    "    bbox = selected_subject['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2\n",
    "\n",
    "    # Calculate crop coordinates\n",
    "    x1 = int(max(0, min(center_x - target_width/2, width - target_width)))\n",
    "    x2 = int(x1 + target_width)\n",
    "\n",
    "    return (x1, 0, x2, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2QzHhCjtpML"
   },
   "source": [
    "The `SmoothingBuffer` class implements advanced smoothing for crop window transitions.\n",
    "\n",
    "Operations:\n",
    "1. Calculates current velocity from position changes\n",
    "2. Maintains velocity history for trend analysis\n",
    "3. Uses weighted averaging for smooth transitions\n",
    "4. Adapts smoothing strength based on motion speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5T7MnB0P1l_a"
   },
   "outputs": [],
   "source": [
    "class SmoothingBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size: int = 15, smoothing_factor: float = 0.7, velocity_weight: float = 0.3):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.velocity_weight = velocity_weight\n",
    "        self.positions = []\n",
    "        self.velocities = []\n",
    "        self.current_position = None\n",
    "        self.last_velocity = 0\n",
    "\n",
    "    def update(self, new_position: Tuple[int, int, int, int]) -> Tuple[int, int, int, int]:\n",
    "        if self.current_position is None:\n",
    "            self.current_position = new_position\n",
    "            return new_position\n",
    "\n",
    "        # Calculate current velocity\n",
    "        current_velocity = new_position[0] - self.current_position[0]\n",
    "\n",
    "        # Update velocity history\n",
    "        self.velocities.append(current_velocity)\n",
    "        if len(self.velocities) > self.buffer_size:\n",
    "            self.velocities.pop(0)\n",
    "\n",
    "        # Calculate smoothed velocity\n",
    "        if self.velocities:\n",
    "            smoothed_velocity = sum(self.velocities) / len(self.velocities)\n",
    "            # Add velocity-based prediction\n",
    "            predicted_x1 = new_position[0] + (smoothed_velocity * self.velocity_weight)\n",
    "        else:\n",
    "            predicted_x1 = new_position[0]\n",
    "\n",
    "        # Update position history\n",
    "        self.positions.append((predicted_x1, new_position[1],\n",
    "                             predicted_x1 + (new_position[2] - new_position[0]), new_position[3]))\n",
    "        if len(self.positions) > self.buffer_size:\n",
    "            self.positions.pop(0)\n",
    "\n",
    "        # Calculate target position with prediction\n",
    "        target_x1 = sum(pos[0] for pos in self.positions) / len(self.positions)\n",
    "        target_x2 = sum(pos[2] for pos in self.positions) / len(self.positions)\n",
    "\n",
    "        # Adaptive smoothing based on velocity\n",
    "        velocity_magnitude = abs(smoothed_velocity) if self.velocities else 0\n",
    "        adaptive_smoothing = max(0.1, self.smoothing_factor - (velocity_magnitude * 0.001))\n",
    "\n",
    "        # Apply smoothing\n",
    "        smooth_x1 = int(adaptive_smoothing * self.current_position[0] +\n",
    "                       (1 - adaptive_smoothing) * target_x1)\n",
    "        smooth_x2 = int(adaptive_smoothing * self.current_position[2] +\n",
    "                       (1 - adaptive_smoothing) * target_x2)\n",
    "\n",
    "        self.current_position = (smooth_x1, new_position[1], smooth_x2, new_position[3])\n",
    "        return self.current_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FnKfQc4t_6R"
   },
   "source": [
    "Main video processing function implementing the content-aware cropping system with proper synchronisation of audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MD_p6s0By7PU"
   },
   "outputs": [],
   "source": [
    "def process_video(input_path: str,\n",
    "                  output_path: str,\n",
    "                  model,\n",
    "                  target_aspect_ratio: float = 9/16,\n",
    "                  buffer_size: int = 30,\n",
    "                  smoothing_factor: float = 0.85,\n",
    "                  switch_threshold: float = 0.3,\n",
    "                  persistence_bonus: float = 0.25) -> None:\n",
    "\n",
    "    # Create temporary directory for frame sequence\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    frames_dir = os.path.join(temp_dir, \"frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Open input video\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(\"Could not open input video\")\n",
    "\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Output dimensions\n",
    "        output_width = int(height * target_aspect_ratio)\n",
    "        output_height = height\n",
    "\n",
    "        # Initialize components for object tracking and smoothing\n",
    "        object_scorer = ObjectScorer(width, height)\n",
    "        subject_tracker = SubjectTracker(switch_threshold=switch_threshold, persistence_bonus=persistence_bonus)\n",
    "        smoother = SmoothingBuffer(buffer_size=buffer_size, smoothing_factor=smoothing_factor)\n",
    "\n",
    "        frame_count = 0\n",
    "        print(\"Processing frames...\")\n",
    "\n",
    "        # Process each frame\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                break  # End of video or frame not read\n",
    "\n",
    "            try:\n",
    "                # Object detection and tracking\n",
    "                detections = detect_objects(frame, model)\n",
    "                raw_x1, raw_y1, raw_x2, raw_y2 = calculate_crop_window(\n",
    "                    frame.shape,\n",
    "                    detections,\n",
    "                    object_scorer,\n",
    "                    subject_tracker,\n",
    "                    target_aspect_ratio\n",
    "                )\n",
    "\n",
    "                # Apply smoothing\n",
    "                x1, y1, x2, y2 = smoother.update((raw_x1, raw_y1, raw_x2, raw_y2))\n",
    "\n",
    "                # Enforce bounds on crop coordinates\n",
    "                x1, y1 = max(0, int(x1)), max(0, int(y1))\n",
    "                x2, y2 = min(width, int(x2)), min(height, int(y2))\n",
    "\n",
    "                # Crop and resize the frame\n",
    "                cropped_frame = frame[y1:y2, x1:x2]\n",
    "                if cropped_frame.size > 0:\n",
    "                    cropped_frame = cv2.resize(cropped_frame, (output_width, output_height))\n",
    "                else:\n",
    "                    # Handle empty frame by using a center crop as fallback\n",
    "                    center = width // 2\n",
    "                    x1 = max(0, center - output_width // 2)\n",
    "                    x2 = min(width, x1 + output_width)\n",
    "                    cropped_frame = frame[:, x1:x2]\n",
    "                    cropped_frame = cv2.resize(cropped_frame, (output_width, output_height))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in frame {frame_count}, using center crop: {str(e)}\")\n",
    "                center = width // 2\n",
    "                x1 = max(0, center - output_width // 2)\n",
    "                x2 = min(width, x1 + output_width)\n",
    "                cropped_frame = frame[:, x1:x2]\n",
    "                cropped_frame = cv2.resize(cropped_frame, (output_width, output_height))\n",
    "\n",
    "            # Save the processed frame\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "            cv2.imwrite(frame_path, cropped_frame)\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if frame_count == 0:\n",
    "            raise Exception(\"No frames were processed\")\n",
    "\n",
    "        print(\"Frame processing complete. Creating video...\")\n",
    "\n",
    "        # Load original video to get exact audio duration\n",
    "        original_video = VideoFileClip(input_path)\n",
    "\n",
    "        # Process frames into a video\n",
    "        frame_files = sorted([os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
    "        processed_video = ImageSequenceClip(frame_files, fps=original_fps)\n",
    "\n",
    "        # Add audio and ensure the duration matches\n",
    "        final_video = processed_video.set_audio(original_video.audio).set_duration(original_video.duration)\n",
    "        final_video.write_videofile(\n",
    "            output_path,\n",
    "            codec='libx264',\n",
    "            audio_codec='aac',\n",
    "            temp_audiofile=os.path.join(temp_dir, \"temp_audio.m4a\"),\n",
    "            remove_temp=True,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video processing: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        try:\n",
    "            original_video.close()\n",
    "            processed_video.close()\n",
    "            final_video.close()\n",
    "        except:\n",
    "            pass\n",
    "        shutil.rmtree(temp_dir)\n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeMnSrrALlKG"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading YOLOv8 model...\")\n",
    "    model = setup_yolo()\n",
    "\n",
    "    input_video = \"2FgBOgck_K0.mp4\" #path to videos\n",
    "    output_video = \"cropped_video10.mp4\"\n",
    "\n",
    "    print(f\"Processing video: {input_video}\")\n",
    "    process_video(\n",
    "        input_video,\n",
    "        output_video,\n",
    "        model,\n",
    "        switch_threshold=0.4,\n",
    "        persistence_bonus=0.3,\n",
    "        buffer_size=20,\n",
    "        smoothing_factor=0.70,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnE_grP_l-kY",
    "outputId": "bb9f9704-40b9-4e80-fca3-5b67c39d4f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 model...\n",
      "Processing video: 2FgBOgck_K0.mp4\n",
      "Processing frames...\n",
      "Processed 30/625 frames\n",
      "Processed 60/625 frames\n",
      "Processed 90/625 frames\n",
      "Processed 120/625 frames\n",
      "Processed 150/625 frames\n",
      "Processed 180/625 frames\n",
      "Processed 210/625 frames\n",
      "Processed 240/625 frames\n",
      "Processed 270/625 frames\n",
      "Processed 300/625 frames\n",
      "Processed 330/625 frames\n",
      "Processed 360/625 frames\n",
      "Processed 390/625 frames\n",
      "Processed 420/625 frames\n",
      "Processed 450/625 frames\n",
      "Processed 480/625 frames\n",
      "Processed 510/625 frames\n",
      "Processed 540/625 frames\n",
      "Processed 570/625 frames\n",
      "Processed 600/625 frames\n",
      "Frame processing complete. Creating video...\n",
      "Moviepy - Building video cropped_video10.mp4.\n",
      "MoviePy - Writing audio in /tmp/tmpd7wsja7p/temp_audio.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video cropped_video10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready cropped_video10.mp4\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
